


# ‚öôÔ∏è Install required packages in the notebook itself
# !pip install --quiet --upgrade pip
!pip install mlflow scikit-learn
!pip install pandas
!pip install numpy
!pip install jupyter
!pip install shap



!pip install scikit-learn




!pip uninstall numpy
!pip install numpy --force-reinstall --upgrade
!pip uninstall shap
!pip install shap --upgrade
!pip install mkl



import mlflow
import mlflow.sklearn
import mlflow.shap
from mlflow.tracking import MlflowClient
import mlflow.utils.environment

import os
import time
import psutil
import shap
import seaborn as sns
import matplotlib.pyplot as plt
import pandas as pd

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score, roc_curve
from sklearn.model_selection import train_test_split
os.environ["MLFLOW_SYSTEM_METRICS_ENABLED"] = "true"



# Set experiment name
# ============================
# üîß Configuration
# ============================
project_dir = os.getcwd()
tracking_dir = os.path.join(project_dir, "mlruns")
mlflow.set_tracking_uri("mlrunlogs/mlflow.db")
# ‚úÖ setting the mlflow logs in the local folder 
print("Tracking URI set to:", mlflow.get_tracking_uri())

mlflow.set_experiment("RandomForest-Iris-CSV")
client = MlflowClient()

# ============================
# üì• Load Data from CSV
# ============================
# Read data from CSV file
df = pd.read_csv("C:/Users/reema/OneDrive/Dokumente/Provenance_tracking_thesis/Provenence-Tracking-Thesis-Research/data/Iris.csv")
# Use the last column as the target variable
X = df.iloc[:, :-1]  # All columns except the last one are features
y = df.iloc[:, -1]   # The last column is the target

# Create a dataset object with mlflow.data
dataset = mlflow.data.from_pandas(df)

# Split the data into train and test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Define the model (RandomForestClassifier)
model = RandomForestClassifier(n_estimators=100, random_state=42)





# Start MLflow run
with mlflow.start_run():

    # Log experiment parameters and metadata
    mlflow.log_param("model", "RandomForestClassifier")
    mlflow.log_param("n_estimators", 100)

    # Track the dataset used in the experiment
    dataset_uri = "URL TO THE DATASET"  # TODO :Replace with actual dataset URI
    mlflow.log_param("dataset_uri", dataset_uri)

    # Tracking data provenance (using metadata schemas like PROV-O or Croissant)
    mlflow.log_param("data_version", "1.0")  # # TODO :Replace with actual dataset URI Example versioning of the dataset
    mlflow.log_param("preprocessing_steps", "StandardScaler applied")

    # Track the system CPU usage during training (resource utilization)
    start_time = time.time()
    cpu_usage_before = psutil.cpu_percent(interval=1)

    # Train the model
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)

    cpu_usage_after = psutil.cpu_percent(interval=1)
    elapsed_time = time.time() - start_time

    # Log metrics (e.g., CPU usage, training time)
    mlflow.log_metric("cpu_usage_before_training", cpu_usage_before)
    mlflow.log_metric("cpu_usage_after_training", cpu_usage_after)
    mlflow.log_metric("elapsed_time", elapsed_time)

    # Log the model
    mlflow.sklearn.log_model(model, "random_forest_model")

    # Evaluate the model on the test set
    y_pred = model.predict(X_test)
    accuracy = accuracy_score(y_test, y_pred)
    confusion = confusion_matrix(y_test, y_pred)
    auc = roc_auc_score(y_test, model.predict_proba(X_test), multi_class="ovr")

    # Log evaluation metrics
    mlflow.log_metric("accuracy", accuracy)
    mlflow.log_metric("roc_auc", auc)

    # Log confusion matrix plot
    plt.figure(figsize=(6, 6))
    sns.heatmap(confusion, annot=True, fmt="d", cmap="Blues", cbar=False)
    plt.title('Confusion Matrix')
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    confusion_path = "confusion_matrix.png"
    plt.savefig(confusion_path)
    mlflow.log_artifact(confusion_path)

    # Log ROC curve plot
    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(X_test)[:, 1], pos_label=1)
    plt.figure(figsize=(6, 6))
    plt.plot(fpr, tpr, color='darkorange', lw=2, label='ROC curve (area = %0.2f)' % auc)
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic')
    roc_path = "roc_curve.png"
    plt.savefig(roc_path)
    mlflow.log_artifact(roc_path)

    # Log feature importances
    feature_importances = model.feature_importances_
    feature_names = X.columns  # Use the column names for features
    feature_importance_df = pd.DataFrame({
        'Feature': feature_names,
        'Importance': feature_importances
    }).sort_values(by='Importance', ascending=False)

    feature_importance_path = "feature_importance.csv"
    # feature_importance_df.to_csv(feature_importance_path, index=False) #uncomment after all integretion
    mlflow.log_artifact(feature_importance_path)

    # Register the model in the model registry
    model_uri = f"runs:/{mlflow.active_run().info.run_id}/random_forest_model"
    mlflow.register_model(model_uri, "RandomForest_Iris_Model")

    # PyFunc Model Example: Define a custom model wrapper for scoring
    class RandomForestModelWrapper(mlflow.pyfunc.PythonModel):
        def load_context(self, context):
            self.model = model

        def predict(self, context, model_input):
            return self.model.predict(model_input)

    # Log the custom PyFunc model
    pyfunc_model_path = "random_forest_pyfunc_model"
    mlflow.pyfunc.log_model(artifact_path=pyfunc_model_path, python_model=RandomForestModelWrapper())



    # Finish the run
    mlflow.end_run()

    # Print model registration info
    print(f"Model registered and deployed at version {mlflow.active_run().info.run_id}")





















